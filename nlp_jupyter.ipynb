{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742a3e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install nltk\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a6f5069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 12:08:16.727603: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-15 12:08:16.839009: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-15 12:08:16.839031: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-15 12:08:17.513005: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-15 12:08:17.513062: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-15 12:08:17.513070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import ast\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89395de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = TweetTokenizer(reduce_len=True)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        self.regex = \"RT (@[A-Za-z0-9_]+)|(@[A-Za-z0-9_]+)|https\\S+|http\\S+|(?<!\\d)[.,;:!?](?!\\d)\"\n",
    "        self.emoji_dict = None\n",
    "        self.stop_word = None\n",
    "        \n",
    "        self.tweet_vec = CountVectorizer(tokenizer=self.tokenize)\n",
    "        self.emoji_vec = CountVectorizer()\n",
    "        \n",
    "        self.make_stop_word()\n",
    "    \n",
    "    def make_stop_word(self):\n",
    "        self.stop_word = set(stopwords.words('english'))\n",
    "        stop_word_symbol = {\"…\", \"’\", \":\", '\"', '-', '️', '&', '“', '(', '/', \"'\", \";\", \"+\", \"*\", \"~\"}\n",
    "        self.stop_word.update(stop_word_symbol)\n",
    "    \n",
    "    def tokenize(self, text): # tokenize the tweets\n",
    "        tknzr = TweetTokenizer()\n",
    "        return tknzr.tokenize(text)\n",
    "    \n",
    "    def vectorize(self, data):      \n",
    "        # Fit the vectorizer on the 'tweet' column\n",
    "        self.tweet_vec.fit(data['tweet'])\n",
    "        \n",
    "        # Transform the 'tweet' column into a numerical representation\n",
    "        tweet_vectors = self.tweet_vec.transform(data['tweet']) # matrix of token counts\n",
    "        \n",
    "        return tweet_vectors\n",
    "            \n",
    "    def vectorize_emojis(self, data):\n",
    "        data['emojis'] = data['emojis'].apply(        \n",
    "            lambda x: ast.literal_eval(x)\n",
    "        )\n",
    "\n",
    "        data['emojis'] = data['emojis'].apply(lambda x: (''.join(x) if len(x) > 0 else 'EMPTY')) # emoji가 꼭 있는 거로 data를 모아야 할 듯\n",
    "\n",
    "        self.emoji_vec.fit(data['emojis']) # Fit the vectorizer to the 'emojis' column\n",
    "\n",
    "        # Transform the 'emojis' column to a numerical representation\n",
    "        emoji_vectors = self.emoji_vec.transform(data['emojis'])\n",
    "        \n",
    "        return emoji_vectors\n",
    "    \n",
    "    def preprocess(self, data):\n",
    "        tweet_vectors = self.vectorize(data)\n",
    "        emoji_vectors = self.vectorize_emojis(data)\n",
    "        \n",
    "        # concatenate the tweet vectors and emoji sequences into a single feature matrix\n",
    "        combined_vec = np.hstack((tweet_vectors, emoji_vectors))\n",
    "        return combined_vec # preprocessed data\n",
    "    \n",
    "dp = DataPreprocessor()\n",
    "data = pd.read_csv('crawl/data.csv')\n",
    "\n",
    "combined_vec = dp.preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35866b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821da27",
   "metadata": {},
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd64878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 12:08:21.566375: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-15 12:08:21.566716: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-15 12:08:21.566737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2022-12-15 12:08:21.567266: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  3/122 [..............................] - ETA: 2:14:44 - loss: 0.8174 - acc: 0.7552"
     ]
    }
   ],
   "source": [
    "target_data = data['label']\n",
    "\n",
    "reg_coeff = 0.01\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Get the unique words in the input data\n",
    "combined_vec_matrix = hstack(combined_vec)\n",
    "input_data_array = combined_vec_matrix.toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data_array, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further try\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(input_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Split the train data into train and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "unique_words = np.unique(X_train)\n",
    "\n",
    "vocab_size = len(unique_words) + 1\n",
    "\n",
    "# Set the total_cnt parameter in the Embedding layer\n",
    "model.add(Embedding(vocab_size, output_dim=100))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l1(reg_coeff), \n",
    "                bias_regularizer=regularizers.l2(reg_coeff)))\n",
    "\n",
    "es_l = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "es_a = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_GRU.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "          batch_size=64,\n",
    "          epochs=20, \n",
    "          verbose=1,\n",
    "          validation_split=0.2, \n",
    "          callbacks=[es_l, es_a, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ebb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test loss: %.4f' % perf[0])\n",
    "print('Test accuracy: %.2f' % (perf[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d1bb6",
   "metadata": {},
   "source": [
    "## KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb42625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_coeff = 0.01\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Get the unique words in the input data\n",
    "combined_vec_matrix = hstack(combined_vec)\n",
    "\n",
    "non_zero_elements = combined_vec_matrix.count_nonzero()\n",
    "\n",
    "vocab_size = non_zero_elements + 1\n",
    "\n",
    "# Set the total_cnt parameter in the Embedding layer\n",
    "model.add(Embedding(vocab_size, output_dim=100))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l1(reg_coeff), \n",
    "                bias_regularizer=regularizers.l2(reg_coeff)))\n",
    "\n",
    "es_l = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "es_a = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_GRU.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c128e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the KFold class\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create a KFold object with 5 folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "X = combined_vec_matrix.toarray()\n",
    "y = target_data\n",
    "\n",
    "# Loop through the folds\n",
    "for train_index, val_index in kf.split(X):\n",
    "    # Get the training and validation data for this fold\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "    \n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_fold, y_train_fold, \n",
    "              batch_size=32, \n",
    "              epochs=5, \n",
    "              verbose=1,\n",
    "              validation_split=0.2,\n",
    "              callbacks=[es_l, es_a, mc])\n",
    "    \n",
    "    # Evaluate the model on the validation data for this fold\n",
    "    val_loss, val_acc = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    print(f'Fold val_loss: {val_loss:.3f}, val_acc: {val_acc:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2129948",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test loss: %.4f' % perf[0])\n",
    "print('Test accuracy: %.2f' % (perf[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51140c1",
   "metadata": {},
   "source": [
    "## Use the pre-trained word2vec model\n",
    "\n",
    "- Hmm..\n",
    "- KFold + pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the pre-trained word2vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('word2vec.txt', binary=False)\n",
    "\n",
    "reg_coeff = 0.01\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Get the unique words in the input data\n",
    "combined_vec_matrix = hstack(combined_vec)\n",
    "non_zero_elements = combined_vec_matrix.count_nonzero()\n",
    "vocab_size = non_zero_elements + 1\n",
    "\n",
    "# Set the total_cnt parameter in the Embedding layer\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[word2vec_model])) # <- Here!!\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l1(reg_coeff), \n",
    "                bias_regularizer=regularizers.l2(reg_coeff)))\n",
    "\n",
    "es_l = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "es_a = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_GRU.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59374b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the KFold class\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create a KFold object with 5 folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "X = combined_vec_matrix.toarray()\n",
    "y = target_data\n",
    "\n",
    "# Loop through the folds\n",
    "for train_index, val_index in kf.split(X):\n",
    "    # Get the training and validation data for this fold\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "    \n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_fold, y_train_fold, \n",
    "              batch_size=32, \n",
    "              epochs=5, \n",
    "              verbose=1,\n",
    "              validation_split=0.2,\n",
    "              callbacks=[es_l, es_a, mc])\n",
    "    \n",
    "    # Evaluate the model on the validation data for this fold\n",
    "    val_loss, val_acc = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    print(f'Fold val_loss: {val_loss:.3f}, val_acc: {val_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc5346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4353fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e77db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ca29870",
   "metadata": {},
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stop_word = set(stopwords.words('english'))\n",
    "        stop_word_symbol = {\"…\", \"’\", \":\", '\"', '-', '️', '&', '“', '(', '/', \"'\", \";\", \"+\", \"*\", \"~\"}\n",
    "        self.stop_word.update(stop_word_symbol)\n",
    "        \n",
    "        self.tokenizer = TweetTokenizer(reduce_len=True)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        self.tk = Tokenizer()\n",
    "        self.total_cnt = 0\n",
    "        self.max_len = 100\n",
    "        \n",
    "        self.regex = \"RT (@[A-Za-z0-9_]+)|(@[A-Za-z0-9_]+)|https\\S+|http\\S+|(?<!\\d)[.,;:!?](?!\\d)\"\n",
    "    \n",
    "    def preprocess(self, data_file, test_file):  \n",
    "        data = pd.read_csv(data_file)\n",
    "        test_data = pd.read_csv(test_file)\n",
    "        data.drop_duplicates(subset=['tweet'], inplace=True)\n",
    "        test_data.drop_duplicates(subset=['tweet'], inplace=True)\n",
    "        \n",
    "        data['tweet'] = data['tweet'].str.replace(self.regex, \"\")\n",
    "        test_data['tweet'] = test_data['tweet'].str.replace(self.regex, \"\")\n",
    "        \n",
    "        data['tokenized'] = data['tweet'].apply(lambda x: [self.lemmatizer.lemmatize(word) for word in self.tokenizer.tokenize(x.lower()) if word not in self.stop_word])\n",
    "        test_data['tokenized'] = test_data['tweet'].apply(lambda x: [self.lemmatizer.lemmatize(word) for word in self.tokenizer.tokenize(x.lower()) if word not in self.stop_word])\n",
    "    \n",
    "        X_data, Y_data = data[['tokenized', 'emojis']].values, data['label'].values\n",
    "        X_test, Y_test = test_data[['tokenized', 'emojis']].values, test_data['label'].values\n",
    "        \n",
    "        self.tk_oov = Tokenizer(self.vocab_size, oov_token='OOV')\n",
    "        self.tk_oov.fit_on_texts(X_data)\n",
    "        \n",
    "        X_data = self.tk_oov.texts_to_sequences(X_data)\n",
    "        X_test = self.tk_oov.texts_to_sequences(X_test)\n",
    "        \n",
    "        X_data = pad_sequences(X_data, maxlen=self.max_len)\n",
    "        X_test = pad_sequences(X_test, maxlen=self.max_len)\n",
    "\n",
    "        return X_data, Y_data, X_test, Y_test, self.total_cnt\n",
    "    \n",
    "    def preprocess_sentence(self, sentence):\n",
    "        col = ['tweet'] # ['tweet', 'emojis']\n",
    "        X_df = pd.DataFrame([sentence], columns=col)\n",
    "\n",
    "        X_df = X_df['tweet'].str.replace(self.regex, \"\")\n",
    "\n",
    "        X_df['tokenized'] = X_df.apply(lambda x: [self.lemmatizer.lemmatize(word) for word in self.tokenizer.tokenize(x.lower()) if word not in self.stop_word])\n",
    "        \n",
    "        X = X_df['tokenized'].values\n",
    "        \n",
    "        X = self.tk_oov.texts_to_sequences(X)\n",
    "        X = pad_sequences(X, maxlen=self.max_len)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0da3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DataPreprocessor()\n",
    "# X_data, Y_data, X_test, Y_test, total_cnt = dp.preprocess(\"./crawl/tweets-new.csv\", \"./crawl/tweets-new.csv\")\n",
    "X_data, Y_data, X_test, Y_test, total_cnt = dp.preprocess(\"./TweetBLM.csv\", \"./crawl/tweets-new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf086d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_coeff = 0.001\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_cnt, 100))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l1(reg_coeff), \n",
    "                bias_regularizer=regularizers.l2(reg_coeff)))\n",
    "\n",
    "# callbacks\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=8)\n",
    "mc = ModelCheckpoint('best_GRU.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9146c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model on the dataset\n",
    "history = model.fit(X_data, Y_data, epochs=20, callbacks=[es, mc], batch_size=64, validation_split=0.2)\n",
    "\n",
    "GRU_model = load_model('best_GRU.h5')\n",
    "\n",
    "# evaluate the model on the test data\n",
    "results = GRU_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9993b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test loss: %.4f' % results[0])\n",
    "print('Test accuracy: %.2f' % (results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0987167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = dp.preprocess_sentence(\"FUCK!!! It's terrible. Boring. Tired. Worst ever.\")\n",
    "good = dp.preprocess_sentence(\"LOVE. Like. happy. happiness. peaceful.\")\n",
    "neutral = dp.preprocess_sentence(\"bitch\")\n",
    "\n",
    "for b in GRU_model.predict(bad): print(b)\n",
    "for g in GRU_model.predict(good): print(g)\n",
    "for n in GRU_model.predict(neutral): print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76353b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_se = dp.preprocess_sentence(\"gross dislike hate you\")\n",
    "# test_se = dp.preprocess_sentence(\"\")\n",
    "GRU_model.predict(test_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef54bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

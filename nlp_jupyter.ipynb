{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742a3e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install nltk\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a6f5069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 14:09:57.297646: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-15 14:09:57.422119: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-15 14:09:57.422142: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-15 14:09:58.139337: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-15 14:09:58.139396: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-15 14:09:58.139404: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89395de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soyeong/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = TweetTokenizer(reduce_len=True)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        self.regex = \"RT (@[A-Za-z0-9_]+)|(@[A-Za-z0-9_]+)|https\\S+|http\\S+|(?<!\\d)[.,;:!?](?!\\d)\"\n",
    "        self.emoji_dict = None\n",
    "        self.stop_word = None\n",
    "        \n",
    "        self.tweet_vec = CountVectorizer(tokenizer=self.tokenize_lemmatize)\n",
    "        self.emoji_vec = CountVectorizer()\n",
    "        \n",
    "        self.make_stop_word()\n",
    "    \n",
    "    def make_stop_word(self):\n",
    "        self.stop_word = set(stopwords.words('english'))\n",
    "        stop_word_symbol = {\"‚Ä¶\", \"‚Äô\", \":\", '\"', '-', 'Ô∏è', '&', '‚Äú', '(', '/', \"'\", \";\", \"+\", \"*\", \"~\"}\n",
    "        self.stop_word.update(stop_word_symbol)\n",
    "    \n",
    "    def tokenize(self, text): # tokenize the tweets\n",
    "        tknzr = TweetTokenizer()\n",
    "        return tknzr.tokenize(text)\n",
    "    \n",
    "    def tokenize_lemmatize(self, text): # tokenize the tweets\n",
    "        tknzr = TweetTokenizer()\n",
    "        tokens= tknzr.tokenize(text)\n",
    "\n",
    "        lemmas = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        return [lemma.lower() for lemma in lemmas]\n",
    "    \n",
    "    def vectorize(self, data):\n",
    "        \n",
    "        # Fit the vectorizer on the 'tweet' column\n",
    "        self.tweet_vec.fit(data['tweet'])\n",
    "        \n",
    "        # Transform the 'tweet' column into a numerical representation\n",
    "        tweet_vectors = self.tweet_vec.transform(data['tweet']) # matrix of token counts\n",
    "        \n",
    "        return tweet_vectors\n",
    "            \n",
    "    def vectorize_emojis(self, data):\n",
    "        data['emojis'] = data['emojis'].apply(        \n",
    "            lambda x: ast.literal_eval(x)\n",
    "        )\n",
    "\n",
    "        data['emojis'] = data['emojis'].apply(lambda x: (''.join(x) if len(x) > 0 else 'EMPTY')) # emojiÍ∞Ä Íº≠ ÏûàÎäî Í±∞Î°ú dataÎ•º Î™®ÏïÑÏïº Ìï† ÎìØ\n",
    "\n",
    "        self.emoji_vec.fit(data['emojis']) # Fit the vectorizer to the 'emojis' column\n",
    "\n",
    "        # Transform the 'emojis' column to a numerical representation\n",
    "        emoji_vectors = self.emoji_vec.transform(data['emojis'])\n",
    "        \n",
    "        return emoji_vectors\n",
    "    \n",
    "    def preprocess(self, data):\n",
    "        tweet_vectors = self.vectorize(data)\n",
    "        emoji_vectors = self.vectorize_emojis(data)\n",
    "        \n",
    "        # concatenate the tweet vectors and emoji sequences into a single feature matrix\n",
    "        combined_vec = np.hstack((tweet_vectors, emoji_vectors))\n",
    "        return combined_vec # preprocessed data\n",
    "    \n",
    "dp = DataPreprocessor()\n",
    "data = pd.read_csv('crawl/data.csv')\n",
    "\n",
    "combined_vec = dp.preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17cd61d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>hate_words</th>\n",
       "      <th>emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üî™ Among Us x @USER üî™ Benoit Blanc‚Äôs next great...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>üî™üî™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I just listed a new collab on @USER created al...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>EMPTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mercury + venus are now in capricorn.this is n...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>EMPTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@USER: Wishing an üëèü§ù prosperous happy birthday...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>üëèü§ùüî•</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER: a little gift for reaching 50k thank u ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>üî™üíó</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label hate_words emojis\n",
       "0  üî™ Among Us x @USER üî™ Benoit Blanc‚Äôs next great...      0         []     üî™üî™\n",
       "1  I just listed a new collab on @USER created al...      0         []  EMPTY\n",
       "2  mercury + venus are now in capricorn.this is n...      0         []  EMPTY\n",
       "3  @USER: Wishing an üëèü§ù prosperous happy birthday...      0         []    üëèü§ùüî•\n",
       "4  @USER: a little gift for reaching 50k thank u ...      0         []     üî™üíó"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a35866b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821da27",
   "metadata": {},
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd64878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 90/243 [==========>...................] - ETA: 3:08:42 - loss: 1.3630 - acc: 0.7306"
     ]
    }
   ],
   "source": [
    "target_data = data['label']\n",
    "\n",
    "reg_coeff = 0.1 # 0.001, 0.01, 0.1, 1, 10\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Get the unique words in the input data\n",
    "combined_vec_matrix = hstack(combined_vec)\n",
    "input_data_array = combined_vec_matrix.toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data_array, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further try\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(input_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Split the train data into train and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "unique_words = np.unique(X_train)\n",
    "\n",
    "vocab_size = len(unique_words) + 1\n",
    "\n",
    "# Set the total_cnt parameter in the Embedding layer\n",
    "model.add(Embedding(vocab_size, output_dim=100))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l1(reg_coeff), \n",
    "                bias_regularizer=regularizers.l2(reg_coeff)))\n",
    "\n",
    "es_l = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "es_a = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_GRU.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "          batch_size=32,\n",
    "          epochs=20, \n",
    "          verbose=1,\n",
    "          validation_split=0.2, \n",
    "          callbacks=[es_l, es_a, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ebb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test loss: %.4f' % perf[0])\n",
    "print('Test accuracy: %.2f' % (perf[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d1bb6",
   "metadata": {},
   "source": [
    "## KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb42625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_coeff = 0.01\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Get the unique words in the input data\n",
    "combined_vec_matrix = hstack(combined_vec)\n",
    "\n",
    "non_zero_elements = combined_vec_matrix.count_nonzero()\n",
    "\n",
    "vocab_size = non_zero_elements + 1\n",
    "\n",
    "# Set the total_cnt parameter in the Embedding layer\n",
    "model.add(Embedding(vocab_size, output_dim=100))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l1(reg_coeff), \n",
    "                bias_regularizer=regularizers.l2(reg_coeff)))\n",
    "\n",
    "es_l = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "es_a = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_GRU.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c128e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the KFold class\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create a KFold object with 5 folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "X = combined_vec_matrix.toarray()\n",
    "y = target_data\n",
    "\n",
    "# Loop through the folds\n",
    "for train_index, val_index in kf.split(X):\n",
    "    # Get the training and validation data for this fold\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "    \n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_fold, y_train_fold, \n",
    "              batch_size=32, \n",
    "              epochs=5, \n",
    "              verbose=1,\n",
    "              validation_split=0.2,\n",
    "              callbacks=[es_l, es_a, mc])\n",
    "    \n",
    "    # Evaluate the model on the validation data for this fold\n",
    "    val_loss, val_acc = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    print(f'Fold val_loss: {val_loss:.3f}, val_acc: {val_acc:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2129948",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test loss: %.4f' % perf[0])\n",
    "print('Test accuracy: %.2f' % (perf[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51140c1",
   "metadata": {},
   "source": [
    "## Use the pre-trained word2vec model\n",
    "\n",
    "- Hmm..\n",
    "- KFold + pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the pre-trained word2vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('word2vec.txt', binary=False)\n",
    "\n",
    "reg_coeff = 0.01\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Get the unique words in the input data\n",
    "combined_vec_matrix = hstack(combined_vec)\n",
    "non_zero_elements = combined_vec_matrix.count_nonzero()\n",
    "vocab_size = non_zero_elements + 1\n",
    "\n",
    "# Set the total_cnt parameter in the Embedding layer\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[word2vec_model])) # <- Here!!\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l1(reg_coeff), \n",
    "                bias_regularizer=regularizers.l2(reg_coeff)))\n",
    "\n",
    "es_l = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "es_a = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_GRU.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59374b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the KFold class\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create a KFold object with 5 folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "X = combined_vec_matrix.toarray()\n",
    "y = target_data\n",
    "\n",
    "# Loop through the folds\n",
    "for train_index, val_index in kf.split(X):\n",
    "    # Get the training and validation data for this fold\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "    \n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_fold, y_train_fold, \n",
    "              batch_size=32, \n",
    "              epochs=5, \n",
    "              verbose=1,\n",
    "              validation_split=0.2,\n",
    "              callbacks=[es_l, es_a, mc])\n",
    "    \n",
    "    # Evaluate the model on the validation data for this fold\n",
    "    val_loss, val_acc = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    print(f'Fold val_loss: {val_loss:.3f}, val_acc: {val_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc5346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4353fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e77db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ca29870",
   "metadata": {},
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stop_word = set(stopwords.words('english'))\n",
    "        stop_word_symbol = {\"‚Ä¶\", \"‚Äô\", \":\", '\"', '-', 'Ô∏è', '&', '‚Äú', '(', '/', \"'\", \";\", \"+\", \"*\", \"~\"}\n",
    "        self.stop_word.update(stop_word_symbol)\n",
    "        \n",
    "        self.tokenizer = TweetTokenizer(reduce_len=True)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        self.tk = Tokenizer()\n",
    "        self.total_cnt = 0\n",
    "        self.max_len = 100\n",
    "        \n",
    "        self.regex = \"RT (@[A-Za-z0-9_]+)|(@[A-Za-z0-9_]+)|https\\S+|http\\S+|(?<!\\d)[.,;:!?](?!\\d)\"\n",
    "    \n",
    "    def preprocess(self, data_file, test_file):  \n",
    "        data = pd.read_csv(data_file)\n",
    "        test_data = pd.read_csv(test_file)\n",
    "        data.drop_duplicates(subset=['tweet'], inplace=True)\n",
    "        test_data.drop_duplicates(subset=['tweet'], inplace=True)\n",
    "        \n",
    "        data['tweet'] = data['tweet'].str.replace(self.regex, \"\")\n",
    "        test_data['tweet'] = test_data['tweet'].str.replace(self.regex, \"\")\n",
    "        \n",
    "        data['tokenized'] = data['tweet'].apply(lambda x: [self.lemmatizer.lemmatize(word) for word in self.tokenizer.tokenize(x.lower()) if word not in self.stop_word])\n",
    "        test_data['tokenized'] = test_data['tweet'].apply(lambda x: [self.lemmatizer.lemmatize(word) for word in self.tokenizer.tokenize(x.lower()) if word not in self.stop_word])\n",
    "    \n",
    "        X_data, Y_data = data[['tokenized', 'emojis']].values, data['label'].values\n",
    "        X_test, Y_test = test_data[['tokenized', 'emojis']].values, test_data['label'].values\n",
    "        \n",
    "        self.tk_oov = Tokenizer(self.vocab_size, oov_token='OOV')\n",
    "        self.tk_oov.fit_on_texts(X_data)\n",
    "        \n",
    "        X_data = self.tk_oov.texts_to_sequences(X_data)\n",
    "        X_test = self.tk_oov.texts_to_sequences(X_test)\n",
    "        \n",
    "        X_data = pad_sequences(X_data, maxlen=self.max_len)\n",
    "        X_test = pad_sequences(X_test, maxlen=self.max_len)\n",
    "\n",
    "        return X_data, Y_data, X_test, Y_test, self.total_cnt\n",
    "    \n",
    "    def preprocess_sentence(self, sentence):\n",
    "        col = ['tweet'] # ['tweet', 'emojis']\n",
    "        X_df = pd.DataFrame([sentence], columns=col)\n",
    "\n",
    "        X_df = X_df['tweet'].str.replace(self.regex, \"\")\n",
    "\n",
    "        X_df['tokenized'] = X_df.apply(lambda x: [self.lemmatizer.lemmatize(word) for word in self.tokenizer.tokenize(x.lower()) if word not in self.stop_word])\n",
    "        \n",
    "        X = X_df['tokenized'].values\n",
    "        \n",
    "        X = self.tk_oov.texts_to_sequences(X)\n",
    "        X = pad_sequences(X, maxlen=self.max_len)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0da3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DataPreprocessor()\n",
    "# X_data, Y_data, X_test, Y_test, total_cnt = dp.preprocess(\"./crawl/tweets-new.csv\", \"./crawl/tweets-new.csv\")\n",
    "X_data, Y_data, X_test, Y_test, total_cnt = dp.preprocess(\"./TweetBLM.csv\", \"./crawl/tweets-new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf086d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_coeff = 0.001\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_cnt, 100))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l1(reg_coeff), \n",
    "                bias_regularizer=regularizers.l2(reg_coeff)))\n",
    "\n",
    "# callbacks\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=8)\n",
    "mc = ModelCheckpoint('best_GRU.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9146c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model on the dataset\n",
    "history = model.fit(X_data, Y_data, epochs=20, callbacks=[es, mc], batch_size=64, validation_split=0.2)\n",
    "\n",
    "GRU_model = load_model('best_GRU.h5')\n",
    "\n",
    "# evaluate the model on the test data\n",
    "results = GRU_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9993b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test loss: %.4f' % results[0])\n",
    "print('Test accuracy: %.2f' % (results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0987167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = dp.preprocess_sentence(\"FUCK!!! It's terrible. Boring. Tired. Worst ever.\")\n",
    "good = dp.preprocess_sentence(\"LOVE. Like. happy. happiness. peaceful.\")\n",
    "neutral = dp.preprocess_sentence(\"bitch\")\n",
    "\n",
    "for b in GRU_model.predict(bad): print(b)\n",
    "for g in GRU_model.predict(good): print(g)\n",
    "for n in GRU_model.predict(neutral): print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76353b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_se = dp.preprocess_sentence(\"gross dislike hate you\")\n",
    "# test_se = dp.preprocess_sentence(\"\")\n",
    "GRU_model.predict(test_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef54bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

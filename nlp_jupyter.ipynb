{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742a3e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install nltk\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a6f5069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 22:04:16.199766: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-14 22:04:16.337163: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-14 22:04:16.337189: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-14 22:04:17.109720: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-14 22:04:17.109881: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-14 22:04:17.109894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import ast\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89395de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = TweetTokenizer(reduce_len=True)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        self.regex = \"RT (@[A-Za-z0-9_]+)|(@[A-Za-z0-9_]+)|https\\S+|http\\S+|(?<!\\d)[.,;:!?](?!\\d)\"\n",
    "        self.emoji_dict = None\n",
    "        self.stop_word = None\n",
    "        \n",
    "        self.tweet_vec = CountVectorizer(tokenizer=self.tokenize)\n",
    "        self.emoji_vec = CountVectorizer()\n",
    "        \n",
    "        self.make_stop_word()\n",
    "    \n",
    "    def make_stop_word(self):\n",
    "        self.stop_word = set(stopwords.words('english'))\n",
    "        stop_word_symbol = {\"…\", \"’\", \":\", '\"', '-', '️', '&', '“', '(', '/', \"'\", \";\", \"+\", \"*\", \"~\"}\n",
    "        self.stop_word.update(stop_word_symbol)\n",
    "    \n",
    "    def tokenize(self, text): # tokenize the tweets\n",
    "            tknzr = TweetTokenizer()\n",
    "            return tknzr.tokenize(text)\n",
    "    \n",
    "    def vectorize(self, data):      \n",
    "        # Fit the vectorizer on the 'tweet' column\n",
    "        self.tweet_vec.fit(data['tweet'])\n",
    "        \n",
    "        # Transform the 'tweet' column into a numerical representation\n",
    "        tweet_vectors = self.tweet_vec.transform(data['tweet']) # matrix of token counts\n",
    "        \n",
    "        return tweet_vectors\n",
    "    \n",
    "    def vectorize_emojis(self, emoji_list):\n",
    "        data['emojis'] = data['emojis'].apply(\n",
    "            lambda x: ast.literal_eval(x)\n",
    "        )\n",
    "        data['emojis'] = data['emojis'].apply(lambda x: (''.join(x) if len(x) > 0 else 'EMPTY')) # emoji가 꼭 있는 거로 data를 모아야 할 듯\n",
    "\n",
    "        self.emoji_vec.fit(data['emojis']) # Fit the vectorizer to the 'emojis' column\n",
    "\n",
    "        # Transform the 'emojis' column to a numerical representation\n",
    "        emoji_vectors = self.emoji_vec.transform(data['emojis'])\n",
    "        \n",
    "        return emoji_vectors\n",
    "    \n",
    "    def preprocess(self, data):\n",
    "        tweet_vectors = self.vectorize(data)\n",
    "        emoji_vectors = self.vectorize_emojis(data)\n",
    "        \n",
    "        # concatenate the tweet vectors and emoji sequences into a single feature matrix\n",
    "        combined_vec = np.hstack((tweet_vectors, emoji_vectors))\n",
    "        return combined_vec # preprocessed data\n",
    "    \n",
    "dp = DataPreprocessor()\n",
    "data = pd.read_csv('crawl/tweets-new.csv')\n",
    "\n",
    "combined_vec = dp.preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35866b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821da27",
   "metadata": {},
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd64878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 22:04:18.762268: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-14 22:04:18.762297: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-14 22:04:18.762318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2022-12-14 22:04:18.762542: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.7238 - acc: 0.9509\n",
      "Epoch 1: val_acc improved from -inf to 0.97059, saving model to best_GRU.h5\n",
      "7/7 [==============================] - 19s 2s/step - loss: 0.7238 - acc: 0.9509 - val_loss: 0.5597 - val_acc: 0.9706\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.4451 - acc: 0.9509\n",
      "Epoch 2: val_acc did not improve from 0.97059\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.4451 - acc: 0.9509 - val_loss: 0.2711 - val_acc: 0.9706\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3596 - acc: 0.9509\n",
      "Epoch 3: val_acc did not improve from 0.97059\n",
      "7/7 [==============================] - 15s 2s/step - loss: 0.3596 - acc: 0.9509 - val_loss: 0.2595 - val_acc: 0.9706\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3211 - acc: 0.9509\n",
      "Epoch 4: val_acc did not improve from 0.97059\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.3211 - acc: 0.9509 - val_loss: 0.2663 - val_acc: 0.9706\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.3167 - acc: 0.9509\n",
      "Epoch 5: val_acc did not improve from 0.97059\n",
      "7/7 [==============================] - 16s 2s/step - loss: 0.3167 - acc: 0.9509 - val_loss: 0.2572 - val_acc: 0.9706\n",
      "Epoch 5: early stopping\n"
     ]
    }
   ],
   "source": [
    "target_data = data['label']\n",
    "\n",
    "reg_coeff = 0.01\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Get the unique words in the input data\n",
    "combined_vec_matrix = hstack(combined_vec)\n",
    "input_data_array = combined_vec_matrix.toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data_array, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further try\n",
    "# # Split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(input_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Split the train data into train and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "unique_words = np.unique(X_train)\n",
    "\n",
    "vocab_size = len(unique_words) + 1\n",
    "\n",
    "# Set the total_cnt parameter in the Embedding layer\n",
    "model.add(Embedding(vocab_size, output_dim=100))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l1(reg_coeff), \n",
    "                bias_regularizer=regularizers.l2(reg_coeff)))\n",
    "\n",
    "es_l = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "es_a = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_GRU.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "          batch_size=64,\n",
    "          epochs=20, \n",
    "          verbose=1,\n",
    "          validation_split=0.2, \n",
    "          callbacks=[es_l, es_a, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c6ebb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 2s 436ms/step - loss: 0.2177 - acc: 0.9844\n",
      "Test loss: 0.2177\n",
      "Test accuracy: 98.44\n"
     ]
    }
   ],
   "source": [
    "perf = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test loss: %.4f' % perf[0])\n",
    "print('Test accuracy: %.2f' % (perf[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d1bb6",
   "metadata": {},
   "source": [
    "## KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb42625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_coeff = 0.01\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Get the unique words in the input data\n",
    "combined_vec_matrix = hstack(combined_vec)\n",
    "\n",
    "non_zero_elements = combined_vec_matrix.count_nonzero()\n",
    "\n",
    "vocab_size = non_zero_elements + 1\n",
    "\n",
    "# Set the total_cnt parameter in the Embedding layer\n",
    "model.add(Embedding(vocab_size, output_dim=100))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l1(reg_coeff), \n",
    "                bias_regularizer=regularizers.l2(reg_coeff)))\n",
    "\n",
    "es_l = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "es_a = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_GRU.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "006c128e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6305 - accuracy: 0.8722WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 27s 2s/step - loss: 0.6305 - accuracy: 0.8722 - val_loss: 0.2349 - val_accuracy: 0.9902\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3430 - accuracy: 0.9459WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 25s 2s/step - loss: 0.3430 - accuracy: 0.9459 - val_loss: 0.2175 - val_accuracy: 0.9902\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3310 - accuracy: 0.9459WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 24s 2s/step - loss: 0.3310 - accuracy: 0.9459 - val_loss: 0.1892 - val_accuracy: 0.9902\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3162 - accuracy: 0.9459WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 24s 2s/step - loss: 0.3162 - accuracy: 0.9459 - val_loss: 0.1898 - val_accuracy: 0.9902\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.3067 - accuracy: 0.9459WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 23s 2s/step - loss: 0.3067 - accuracy: 0.9459 - val_loss: 0.1748 - val_accuracy: 0.9902\n",
      "Epoch 5: early stopping\n",
      "Fold val_loss: 0.190, val_acc: 0.984\n",
      "Epoch 1/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.9607WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 27s 2s/step - loss: 0.2620 - accuracy: 0.9607 - val_loss: 0.1238 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2484 - accuracy: 0.9607WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 24s 2s/step - loss: 0.2484 - accuracy: 0.9607 - val_loss: 0.1240 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2417 - accuracy: 0.9607WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 25s 2s/step - loss: 0.2417 - accuracy: 0.9607 - val_loss: 0.1213 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2390 - accuracy: 0.9607WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 24s 2s/step - loss: 0.2390 - accuracy: 0.9607 - val_loss: 0.1208 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2309 - accuracy: 0.9607WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 24s 2s/step - loss: 0.2309 - accuracy: 0.9607 - val_loss: 0.1046 - val_accuracy: 1.0000\n",
      "Epoch 5: early stopping\n",
      "Fold val_loss: 0.332, val_acc: 0.930\n",
      "Epoch 1/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2716 - accuracy: 0.9461WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 27s 2s/step - loss: 0.2716 - accuracy: 0.9461 - val_loss: 0.1448 - val_accuracy: 0.9902\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2643 - accuracy: 0.9461WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 24s 2s/step - loss: 0.2643 - accuracy: 0.9461 - val_loss: 0.1447 - val_accuracy: 0.9902\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2603 - accuracy: 0.9461WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 24s 2s/step - loss: 0.2603 - accuracy: 0.9461 - val_loss: 0.1384 - val_accuracy: 0.9902\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2593 - accuracy: 0.9461WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 24s 2s/step - loss: 0.2593 - accuracy: 0.9461 - val_loss: 0.1233 - val_accuracy: 0.9902\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2527 - accuracy: 0.9461WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 24s 2s/step - loss: 0.2527 - accuracy: 0.9461 - val_loss: 0.1446 - val_accuracy: 0.9902\n",
      "Epoch 5: early stopping\n",
      "Fold val_loss: 0.157, val_acc: 0.984\n",
      "Epoch 1/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2107 - accuracy: 0.9608WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 28s 2s/step - loss: 0.2107 - accuracy: 0.9608 - val_loss: 0.1038 - val_accuracy: 0.9902\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2067 - accuracy: 0.9608WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 30s 2s/step - loss: 0.2067 - accuracy: 0.9608 - val_loss: 0.1174 - val_accuracy: 0.9902\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1999 - accuracy: 0.9608WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 27s 2s/step - loss: 0.1999 - accuracy: 0.9608 - val_loss: 0.1073 - val_accuracy: 0.9902\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1990 - accuracy: 0.9608WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 26s 2s/step - loss: 0.1990 - accuracy: 0.9608 - val_loss: 0.1036 - val_accuracy: 0.9902\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.1973 - accuracy: 0.9608WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 26s 2s/step - loss: 0.1973 - accuracy: 0.9608 - val_loss: 0.1097 - val_accuracy: 0.9902\n",
      "Epoch 5: early stopping\n",
      "Fold val_loss: 0.267, val_acc: 0.937\n",
      "Epoch 1/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2282 - accuracy: 0.9510WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 28s 2s/step - loss: 0.2282 - accuracy: 0.9510 - val_loss: 0.1119 - val_accuracy: 0.9902\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2259 - accuracy: 0.9510WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 26s 2s/step - loss: 0.2259 - accuracy: 0.9510 - val_loss: 0.1123 - val_accuracy: 0.9902\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2248 - accuracy: 0.9510WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 25s 2s/step - loss: 0.2248 - accuracy: 0.9510 - val_loss: 0.1168 - val_accuracy: 0.9902\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2252 - accuracy: 0.9510WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 25s 2s/step - loss: 0.2252 - accuracy: 0.9510 - val_loss: 0.1133 - val_accuracy: 0.9902\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.2240 - accuracy: 0.9510WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "13/13 [==============================] - 25s 2s/step - loss: 0.2240 - accuracy: 0.9510 - val_loss: 0.1161 - val_accuracy: 0.9902\n",
      "Epoch 5: early stopping\n",
      "Epoch 5: early stopping\n",
      "Fold val_loss: 0.176, val_acc: 0.969\n"
     ]
    }
   ],
   "source": [
    "# Import the KFold class\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create a KFold object with 5 folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "X = combined_vec_matrix.toarray()\n",
    "y = target_data\n",
    "\n",
    "# Loop through the folds\n",
    "for train_index, val_index in kf.split(X):\n",
    "    # Get the training and validation data for this fold\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "    \n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_fold, y_train_fold, \n",
    "              batch_size=32, \n",
    "              epochs=5, \n",
    "              verbose=1,\n",
    "              validation_split=0.2,\n",
    "              callbacks=[es_l, es_a, mc])\n",
    "    \n",
    "    # Evaluate the model on the validation data for this fold\n",
    "    val_loss, val_acc = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    print(f'Fold val_loss: {val_loss:.3f}, val_acc: {val_acc:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2129948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 2s 511ms/step - loss: 0.1321 - accuracy: 0.9844\n",
      "Test loss: 0.1321\n",
      "Test accuracy: 98.44\n"
     ]
    }
   ],
   "source": [
    "perf = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test loss: %.4f' % perf[0])\n",
    "print('Test accuracy: %.2f' % (perf[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51140c1",
   "metadata": {},
   "source": [
    "## Use the pre-trained word2vec model\n",
    "\n",
    "- Hmm..\n",
    "- KFold + pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dfd673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the pre-trained word2vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('word2vec.txt', binary=False)\n",
    "\n",
    "reg_coeff = 0.01\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Get the unique words in the input data\n",
    "combined_vec_matrix = hstack(combined_vec)\n",
    "non_zero_elements = combined_vec_matrix.count_nonzero()\n",
    "vocab_size = non_zero_elements + 1\n",
    "\n",
    "# Set the total_cnt parameter in the Embedding layer\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[word2vec_model])) # <- Here!!\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l1(reg_coeff), \n",
    "                bias_regularizer=regularizers.l2(reg_coeff)))\n",
    "\n",
    "es_l = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "es_a = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_GRU.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59374b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the KFold class\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create a KFold object with 5 folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "X = combined_vec_matrix.toarray()\n",
    "y = target_data\n",
    "\n",
    "# Loop through the folds\n",
    "for train_index, val_index in kf.split(X):\n",
    "    # Get the training and validation data for this fold\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "    \n",
    "    # Compile and train the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_fold, y_train_fold, \n",
    "              batch_size=32, \n",
    "              epochs=5, \n",
    "              verbose=1,\n",
    "              validation_split=0.2,\n",
    "              callbacks=[es_l, es_a, mc])\n",
    "    \n",
    "    # Evaluate the model on the validation data for this fold\n",
    "    val_loss, val_acc = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    print(f'Fold val_loss: {val_loss:.3f}, val_acc: {val_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc5346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4353fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e77db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ca29870",
   "metadata": {},
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2161c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stop_word = set(stopwords.words('english'))\n",
    "        stop_word_symbol = {\"…\", \"’\", \":\", '\"', '-', '️', '&', '“', '(', '/', \"'\", \";\", \"+\", \"*\", \"~\"}\n",
    "        self.stop_word.update(stop_word_symbol)\n",
    "        \n",
    "        self.tokenizer = TweetTokenizer(reduce_len=True)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        self.tk = Tokenizer()\n",
    "        self.total_cnt = 0\n",
    "        self.max_len = 100\n",
    "        \n",
    "        self.regex = \"RT (@[A-Za-z0-9_]+)|(@[A-Za-z0-9_]+)|https\\S+|http\\S+|(?<!\\d)[.,;:!?](?!\\d)\"\n",
    "    \n",
    "    def preprocess(self, data_file, test_file):  \n",
    "        data = pd.read_csv(data_file)\n",
    "        test_data = pd.read_csv(test_file)\n",
    "        data.drop_duplicates(subset=['tweet'], inplace=True)\n",
    "        test_data.drop_duplicates(subset=['tweet'], inplace=True)\n",
    "        \n",
    "        data['tweet'] = data['tweet'].str.replace(self.regex, \"\")\n",
    "        test_data['tweet'] = test_data['tweet'].str.replace(self.regex, \"\")\n",
    "        \n",
    "        data['tokenized'] = data['tweet'].apply(lambda x: [self.lemmatizer.lemmatize(word) for word in self.tokenizer.tokenize(x.lower()) if word not in self.stop_word])\n",
    "        test_data['tokenized'] = test_data['tweet'].apply(lambda x: [self.lemmatizer.lemmatize(word) for word in self.tokenizer.tokenize(x.lower()) if word not in self.stop_word])\n",
    "    \n",
    "        X_data, Y_data = data[['tokenized', 'emojis']].values, data['label'].values\n",
    "        X_test, Y_test = test_data[['tokenized', 'emojis']].values, test_data['label'].values\n",
    "        \n",
    "        self.tk_oov = Tokenizer(self.vocab_size, oov_token='OOV')\n",
    "        self.tk_oov.fit_on_texts(X_data)\n",
    "        \n",
    "        X_data = self.tk_oov.texts_to_sequences(X_data)\n",
    "        X_test = self.tk_oov.texts_to_sequences(X_test)\n",
    "        \n",
    "        X_data = pad_sequences(X_data, maxlen=self.max_len)\n",
    "        X_test = pad_sequences(X_test, maxlen=self.max_len)\n",
    "\n",
    "        return X_data, Y_data, X_test, Y_test, self.total_cnt\n",
    "    \n",
    "    def preprocess_sentence(self, sentence):\n",
    "        col = ['tweet'] # ['tweet', 'emojis']\n",
    "        X_df = pd.DataFrame([sentence], columns=col)\n",
    "\n",
    "        X_df = X_df['tweet'].str.replace(self.regex, \"\")\n",
    "\n",
    "        X_df['tokenized'] = X_df.apply(lambda x: [self.lemmatizer.lemmatize(word) for word in self.tokenizer.tokenize(x.lower()) if word not in self.stop_word])\n",
    "        \n",
    "        X = X_df['tokenized'].values\n",
    "        \n",
    "        X = self.tk_oov.texts_to_sequences(X)\n",
    "        X = pad_sequences(X, maxlen=self.max_len)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd0da3cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m dp \u001b[38;5;241m=\u001b[39m DataPreprocessor()\n\u001b[0;32m----> 2\u001b[0m X_data, Y_data, X_test, Y_test, total_cnt \u001b[38;5;241m=\u001b[39m \u001b[43mdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./crawl/tweets-new.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./crawl/tweets-new.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mDataPreprocessor.preprocess\u001b[0;34m(self, data_file, test_file)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_cnt \u001b[38;5;241m-\u001b[39m rare_cnt \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtk_oov \u001b[38;5;241m=\u001b[39m Tokenizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size, oov_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOOV\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtk_oov\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_on_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m X_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtk_oov\u001b[38;5;241m.\u001b[39mtexts_to_sequences(X_data)\n\u001b[1;32m     49\u001b[0m X_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtk_oov\u001b[38;5;241m.\u001b[39mtexts_to_sequences(X_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/preprocessing/text.py:293\u001b[0m, in \u001b[0;36mTokenizer.fit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 293\u001b[0m         seq \u001b[38;5;241m=\u001b[39m \u001b[43mtext_to_word_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m            \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m         seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer(text)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/preprocessing/text.py:74\u001b[0m, in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Converts a text to a sequence of words (or tokens).\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mDeprecated: `tf.keras.preprocessing.text.text_to_word_sequence` does not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    A list of words (or tokens).\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 74\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m \u001b[43minput_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     76\u001b[0m translate_dict \u001b[38;5;241m=\u001b[39m {c: split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m filters}\n\u001b[1;32m     77\u001b[0m translate_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(translate_dict)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "dp = DataPreprocessor()\n",
    "# X_data, Y_data, X_test, Y_test, total_cnt = dp.preprocess(\"./crawl/tweets-new.csv\", \"./crawl/tweets-new.csv\")\n",
    "X_data, Y_data, X_test, Y_test, total_cnt = dp.preprocess(\"./TweetBLM.csv\", \"./crawl/tweets-new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf086d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_coeff = 0.001\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_cnt, 100))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1, activation='sigmoid',\n",
    "                kernel_regularizer=regularizers.l1(reg_coeff), \n",
    "                bias_regularizer=regularizers.l2(reg_coeff)))\n",
    "\n",
    "# callbacks\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=8)\n",
    "mc = ModelCheckpoint('best_GRU.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f9146c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.4761 - acc: 0.8267\n",
      "Epoch 1: val_acc improved from -inf to 0.07474, saving model to best_GRU.h5\n",
      "115/115 [==============================] - 12s 86ms/step - loss: 0.4761 - acc: 0.8267 - val_loss: 1.3497 - val_acc: 0.0747\n",
      "Epoch 2/20\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.2845 - acc: 0.8830\n",
      "Epoch 2: val_acc improved from 0.07474 to 0.38789, saving model to best_GRU.h5\n",
      "115/115 [==============================] - 9s 82ms/step - loss: 0.2845 - acc: 0.8830 - val_loss: 1.2608 - val_acc: 0.3879\n",
      "Epoch 3/20\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.1490 - acc: 0.9519\n",
      "Epoch 3: val_acc improved from 0.38789 to 0.40153, saving model to best_GRU.h5\n",
      "115/115 [==============================] - 10s 83ms/step - loss: 0.1490 - acc: 0.9519 - val_loss: 1.7577 - val_acc: 0.4015\n",
      "Epoch 4/20\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0932 - acc: 0.9727\n",
      "Epoch 4: val_acc improved from 0.40153 to 0.40316, saving model to best_GRU.h5\n",
      "115/115 [==============================] - 10s 83ms/step - loss: 0.0932 - acc: 0.9727 - val_loss: 2.5018 - val_acc: 0.4032\n",
      "Epoch 5/20\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0702 - acc: 0.9825\n",
      "Epoch 5: val_acc did not improve from 0.40316\n",
      "115/115 [==============================] - 9s 82ms/step - loss: 0.0702 - acc: 0.9825 - val_loss: 2.9507 - val_acc: 0.3530\n",
      "Epoch 6/20\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0550 - acc: 0.9869\n",
      "Epoch 6: val_acc did not improve from 0.40316\n",
      "115/115 [==============================] - 9s 81ms/step - loss: 0.0550 - acc: 0.9869 - val_loss: 3.0710 - val_acc: 0.3426\n",
      "Epoch 7/20\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0457 - acc: 0.9895\n",
      "Epoch 7: val_acc did not improve from 0.40316\n",
      "115/115 [==============================] - 10s 83ms/step - loss: 0.0457 - acc: 0.9895 - val_loss: 3.3311 - val_acc: 0.3710\n",
      "Epoch 8/20\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0414 - acc: 0.9905\n",
      "Epoch 8: val_acc improved from 0.40316 to 0.42390, saving model to best_GRU.h5\n",
      "115/115 [==============================] - 9s 81ms/step - loss: 0.0414 - acc: 0.9905 - val_loss: 2.7251 - val_acc: 0.4239\n",
      "Epoch 9/20\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0358 - acc: 0.9922\n",
      "Epoch 9: val_acc did not improve from 0.42390\n",
      "115/115 [==============================] - 10s 83ms/step - loss: 0.0358 - acc: 0.9922 - val_loss: 3.0926 - val_acc: 0.3841\n",
      "Epoch 10/20\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.0345 - acc: 0.9922\n",
      "Epoch 10: val_acc did not improve from 0.42390\n",
      "115/115 [==============================] - 10s 84ms/step - loss: 0.0345 - acc: 0.9922 - val_loss: 2.9326 - val_acc: 0.3546\n",
      "Epoch 10: early stopping\n",
      "16/16 [==============================] - 1s 26ms/step - loss: 1.0806 - acc: 0.7382\n"
     ]
    }
   ],
   "source": [
    "# train the model on the dataset\n",
    "history = model.fit(X_data, Y_data, epochs=20, callbacks=[es, mc], batch_size=64, validation_split=0.2)\n",
    "\n",
    "GRU_model = load_model('best_GRU.h5')\n",
    "\n",
    "# evaluate the model on the test data\n",
    "results = GRU_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9993b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.0806\n",
      "Test accuracy: 73.82\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: %.4f' % results[0])\n",
    "print('Test accuracy: %.2f' % (results[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0987167b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "[0.9096677]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "[0.00550769]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[0.42200926]\n"
     ]
    }
   ],
   "source": [
    "bad = dp.preprocess_sentence(\"FUCK!!! It's terrible. Boring. Tired. Worst ever.\")\n",
    "good = dp.preprocess_sentence(\"LOVE. Like. happy. happiness. peaceful.\")\n",
    "neutral = dp.preprocess_sentence(\"bitch\")\n",
    "\n",
    "for b in GRU_model.predict(bad): print(b)\n",
    "for g in GRU_model.predict(good): print(g)\n",
    "for n in GRU_model.predict(neutral): print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "76353b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9158893]], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_se = dp.preprocess_sentence(\"gross dislike hate you\")\n",
    "# test_se = dp.preprocess_sentence(\"\")\n",
    "GRU_model.predict(test_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef54bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
